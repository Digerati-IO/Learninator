<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Source: entities/AgentRLTD.js</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Source: entities/AgentRLTD.js</h1>

    



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>var AgentRLTD = AgentRLTD || {};

(function (global) {
    "use strict";

    /**
     * Initialize the AgentRLTD
     * @name AgentRLTD
     * @extends Agent
     * @constructor
     *
     * @param {Vec} position - The x, y location
     * @param {agentOpts} opts - The Agent options
     * @returns {AgentRLTD}
     */
    function AgentRLTD(position, opts) {
        var _this = this;

        Agent.call(this, position, opts);

        this.nStepsHistory = [];
        this.nStepsCounter = 0;
        this.nflot = 1000;
        this.score = 0;

        this.brainOpts = Utility.getOpt(opts, 'spec', {
            update: 'qlearn', // 'qlearn' or 'sarsa'
            gamma: 0.9, // discount factor, [0, 1)
            epsilon: 0.2, // initial epsilon for epsilon-greedy policy, [0, 1)
            alpha: 0.1, // value function learning rate
            lambda: 0.9, // eligibility trace decay, [0,1). 0 = no eligibility traces
            replacingTraces: true, // use replacing or accumulating traces
            planN: 50, // number of planning steps per iteration. 0 = no planning
            smoothPolicyUpdate: true, // non-standard, updates policy smoothly to follow max_a Q
            beta: 0.1 // learning rate for smooth policy update
        });

        if (!this.worker) {
            this.brain = new TDAgent(_this.env, this.brainOpts);
            this.state = _this.env.startState();

            _this.env.reset();

            return this;
        } else {
            this.post = function (cmd, input) {
                this.brain.postMessage({target: 'TD', cmd: cmd, input: input});
            };

            var jEnv = Utility.stringify(_this.env),
                jOpts = Utility.stringify(_this.brainOpts);

            this.brain = new Worker('js/lib/external/rl.js');
            this.brain.onmessage = function (e) {
                var data = e.data;
                switch (data.cmd) {
                case 'init':
                    if (data.msg === 'complete') {
                        _this.state = _this.env.startState();

                        _this.env.reset();
                    }
                    break;
                case 'act':
                    if (data.msg === 'complete') {
                        // run it through environment dynamics
                        var obs = _this.sampleNextState(_this.state, data.input);

                        // allow opportunity for the agent to learn
                        _this.brain.postMessage({cmd: 'learn', input: obs.r});
                    }
                    break;
                case 'learn':
                    if (data.msg === 'complete') {
                        _this.Rarr[_this.state] = obs.r;

                        // evolve environment to next state
                        _this.state = obs.ns;
                        _this.gridLocation = _this.world.grid.getCellAt(_this.sToX(_this.state), _this.sToY(_this.state));

                        let x = _this.gridLocation.coords.bottom.right.x - (_this.world.grid.cellWidth / 2),
                            y = _this.gridLocation.coords.bottom.right.y - (_this.world.grid.cellHeight / 2);
                        _this.position.set(x, y);

                        _this.nStepsCounter += 1;
                        if (typeof obs.resetEpisode !== 'undefined') {
                            _this.score += 1;
                            _this.brain.postMessage({cmd: 'resetEpisode'});
                            // record the reward achieved
                            if (_this.nStepsHistory.length >= _this.nflot) {
                                _this.nStepsHistory = _this.nStepsHistory.slice(1);
                            }
                            _this.nStepsHistory.push(_this.nStepsCounter);
                            _this.nStepsCounter = 0;

                            _this.gridLocation = _this.world.grid.getCellAt(0, 0);
                            _this.position.set(_this.world.grid.cellWidth / 2, _this.world.grid.cellHeight / 2);
                        }
                    }
                    break;
                default:
                    console.log('Unknown command: ' + data.cmd + ' message:' + data.msg);
                    break;
                }
            };

            this.brain.post('init', {env: jEnv, opts: jOpts});
        }
    }

    AgentRLTD.prototype = Object.create(Agent.prototype);
    AgentRLTD.prototype.constructor = Agent;

    /**
     * Agent's chance to act on the world
     * @param {Object} world
     */
    AgentRLTD.prototype.tick = function (world) {
        this.world = world;
        if (!this.worker) {
            // ask agent for an action
            var a = this.brain.act(this.state),
            // run it through environment dynamics
                obs = this.sampleNextState(this.state, a);

            // allow opportunity for the agent to learn
            this.brain.learn(obs.r);
            this.Rarr[this.state] = obs.r;

            // evolve environment to next state
            this.state = obs.ns;
            this.gridLocation = this.world.grid.getCellAt(this.sToX(this.state), this.sToY(this.state));

            let x = this.gridLocation.coords.bottom.right.x - (this.world.grid.cellWidth / 2),
                y = this.gridLocation.coords.bottom.right.y - (this.world.grid.cellHeight / 2);
            this.position.set(x, y);

            this.nStepsCounter += 1;
            if (typeof obs.resetEpisode !== 'undefined') {
                this.score += 1;
                this.brain.resetEpisode();
                // record the reward achieved
                if (this.nStepsHistory.length >= this.nflot) {
                    this.nStepsHistory = this.nStepsHistory.slice(1);
                }
                this.nStepsHistory.push(this.nStepsCounter);
                this.nStepsCounter = 0;

                this.gridLocation = this.world.grid.getCellAt(0, 0);
                this.position.set(this.world.grid.cellWidth / 2, this.world.grid.cellHeight / 2);
            }
        } else {
            this.post('act', this.state);
        }
    };

    global.AgentRLTD = AgentRLTD;

}(this));

</code></pre>
        </article>
    </section>




</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Classes</h3><ul><li><a href="-_anonymous_-WaterWorldGA-Trainer.html">Trainer</a></li><li><a href="Agent.html">Agent</a></li><li><a href="AgentGA.html">AgentGA</a></li><li><a href="AgentRLDQN.html">AgentRLDQN</a></li><li><a href="AgentRLTD.html">AgentRLTD</a></li><li><a href="AgentTD.html">AgentTD</a></li><li><a href="Bitmap.html">Bitmap</a></li><li><a href="Button.html">Button</a></li><li><a href="Camera.html">Camera</a></li><li><a href="Cell.html">Cell</a></li><li><a href="Chromosome.html">Chromosome</a></li><li><a href="CollisionDetector.html">CollisionDetector</a></li><li><a href="datGUI.html">datGUI</a></li><li><a href="DeterministPG.html">DeterministPG</a></li><li><a href="DPAgent.html">DPAgent</a></li><li><a href="DQNAgent.html">DQNAgent</a></li><li><a href="Entity.html">Entity</a></li><li><a href="EntityRLDQN.html">EntityRLDQN</a></li><li><a href="ESPNet.html">ESPNet</a></li><li><a href="ESPTrainer.html">ESPTrainer</a></li><li><a href="Experience.html">Experience</a></li><li><a href="Eye.html">Eye</a></li><li><a href="GATrainer.html">GATrainer</a></li><li><a href="Graph.html">Graph</a></li><li><a href="Grid.html">Grid</a></li><li><a href="GridWorld.html">GridWorld</a></li><li><a href="Interactions.html">Interactions</a></li><li><a href="Map.html">Map</a></li><li><a href="Mat.html">Mat</a></li><li><a href="Maze.html">Maze</a></li><li><a href="MazeWorld.html">MazeWorld</a></li><li><a href="Menu.html">Menu</a></li><li><a href="Player.html">Player</a></li><li><a href="PuckWorld.html">PuckWorld</a></li><li><a href="QuadTree.html">QuadTree</a></li><li><a href="RecurrentReinforceAgent.html">RecurrentReinforceAgent</a></li><li><a href="RewardGraph.html">RewardGraph</a></li><li><a href="SimpleReinforceAgent.html">SimpleReinforceAgent</a></li><li><a href="Solver.html">Solver</a></li><li><a href="TDAgent.html">TDAgent</a></li><li><a href="TDBrain.html">TDBrain</a></li><li><a href="UI.html">UI</a></li><li><a href="Vec.html">Vec</a></li><li><a href="Wall.html">Wall</a></li><li><a href="WaterWorld.html">WaterWorld</a></li><li><a href="WaterWorldEX.html">WaterWorldEX</a></li><li><a href="WaterWorldGA.html">WaterWorldGA</a></li><li><a href="Window.html">Window</a></li><li><a href="World.html">World</a></li></ul><h3>Global</h3><ul><li><a href="global.html#clear">clear</a></li><li><a href="global.html#clone">clone</a></li><li><a href="global.html#divide">divide</a></li><li><a href="global.html#findInsertNode">findInsertNode</a></li><li><a href="global.html#findOverlappingNodes">findOverlappingNodes</a></li><li><a href="global.html#insert">insert</a></li><li><a href="global.html#retrieve">retrieve</a></li><li><a href="global.html#Utility">Utility</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc3/jsdoc">JSDoc 3.3.3</a> on Tue Oct 27 2015 14:32:58 GMT-0700 (PDT)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>
